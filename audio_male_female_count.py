# -*- coding: utf-8 -*-
"""Audio_Male_female_count.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pUZ-EElMlxQ7OSC7KsWroxpo8W3OiDFT
"""



from google.colab import drive
drive.mount('/content/drive')

#reading the zip file
import os
import zipfile

path = '/content/drive/MyDrive/dataset_audio.zip'
with zipfile.ZipFile(path, 'r') as zip:
  zip.extractall()

#number of audio files in the dataset
dataset_path = '/content/test'
len(os.listdir(dataset_path))

#importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import librosa
import librosa.display

os.listdir(dataset_path)[:5]

#reading a audio file
file = os.path.join(dataset_path, '0_005682.wav')
print(file)
audio, sample_rate = librosa.load(file)
print("audio: ", audio, '\n')
print("sample_rate: ", sample_rate)

librosa.display.waveplot(audio)

#converting audio to features that can be used in the models
import json

X_train = []
Y_train = []
X_train2 = []
for file in os.listdir(dataset_path):
  #checking if file is json or wave
  temp = file.split('.')
  if temp[1]  == 'json':
    continue
  #reading audio
  audio, sample_rate = librosa.load(os.path.join(dataset_path, file))
  #extracting features
  mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=50)
  X_train2.append(mfccs_features)
  scaled_features = np.mean(mfccs_features.T, axis=0)
  #appending compressed 1D features to Xtrain
  X_train.append(scaled_features)
  #extracting labels
  file_json = temp[0] + '.json'
  file_json = open(os.path.join(dataset_path, file_json), )
  labels = json.load(file_json)
  #counting number of males and females in a audio
  Male = 0
  Female = 0
  for i in range(len(labels)):
    if labels[i]['sex'] == 'F':
      Female += 1
    else:
      Male += 1
  Y_train.append((Male, Female))

#adding my own data and uploading files thorugh the device
from google.colab import  files
for i in range(3):
  files.upload()

#adding my own data to the training data and labels

def extract_features(file):
  audio, sample_rate = librosa.load(file)
  mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=50)
  scaled_features = np.mean(mfccs_features.T, axis=0)
  return scaled_features

features1 = extract_features('audio1.wav')
X_train = list(X_train)
Y_train = list(Y_train)
X_train.append(features1)
Y_train.append((1, 0))


features2 = extract_features('audio2.wav')
X_train.append(features2)
Y_train.append((1, 1))


features3 = extract_features('audio3.wav')
X_train.append(features3)
Y_train.append((0, 0))

#saving training features and labels
data = np.array(X_train)
np.savez("x_train", data)

data = np.array(X_train2)
np.savez("x_train2", data)

data = np.array(Y_train)
np.savez("y_train", data)



import tensorflow as tf
from tensorflow.keras import layers

X_train = tf.convert_to_tensor(X_train)
Y_train = tf.convert_to_tensor(Y_train)
X_train = tf.keras.utils.normalize(X_train, axis=1)

#model architecture

model = tf.keras.Sequential()

model.add(tf.keras.Input(shape=(50)))
model.add(layers.Dense(512, activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Dense(256, activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(2))


model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy'])

model.summary()

X_train[0].shape
from sklearn.model_selection import train_test_split
X_train = np.array(X_train)
Y_train = np.array(Y_train)
x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.1)

x_train.shape, y_train.shape

x_train = tf.convert_to_tensor(x_train)
y_train = tf.convert_to_tensor(y_train)

earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', 
                      restore_best_weights=True, patience=10)
history = model.fit(x=x_train, y=y_train, validation_split=0.1, 
                    epochs=30, batch_size=16, callbacks=[earlystopping])

df = pd.DataFrame(history.history)
df.plot()
plt.legend()

x_test = tf.convert_to_tensor(x_test)
y_test = tf.convert_to_tensor(y_test)

model.evaluate(x_test, y_test)
#getting 86% accuracy on test data

from google.colab import files
files.upload()

#experimenting with my own data
def audio_to_features(file):
  audio, sample_rate = librosa.load(file)
  mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=50)
  scaled_features = np.mean(mfccs_features.T, axis=0)
  x = tf.convert_to_tensor(scaled_features)
  x = tf.keras.utils.normalize(x, axis=0)
  return x

model.predict(audio_to_features('/content/my_audio.wav'))

** CNN based architecture using spectrogram of a wav file **

"""## CNN based architecture using spectrogram of a wav file

"""

from scipy import signal
from scipy.io import wavfile
from PIL import Image as im
import cv2

def wav_to_spectrogram(file):
  sample_rate, samples = wavfile.read(file)
  frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)
  spectrogram = cv2.resize(spectrogram, dsize=(128, 128))
  # norm_img = (spectrogram - np.min(spectrogram)) / (np.max(spectrogram) - np.min(spectrogram))
  return spectrogram
  
x = wav_to_spectrogram(os.path.join(dataset_path, '1_42d601.wav'))
print(x)

plt.imshow(x)

X_img = []
Y_img = []

for file in os.listdir(dataset_path):
  #checking if file is json or wave
  temp = file.split('.')
  if temp[1]  == 'json':
    continue
  
  img = wav_to_spectrogram(os.path.join(dataset_path, file))
  X_img.append(img)
  #extracting labels
  file_json = temp[0] + '.json'
  file_json = open(os.path.join(dataset_path, file_json), )
  labels = json.load(file_json)
  #counting number of males and females in a audio
  Male = 0
  Female = 0
  for i in range(len(labels)):
    if labels[i]['sex'] == 'F':
      Female += 1
    else:
      Male += 1
  Y_img.append((Male, Female))

plt.imshow(X_img[0])
plt.title(f'Males: {Y_img[0][0]} Females: {Y_img[0][1]}')

X_img.shape

#CNN model
Model = tf.keras.Sequential()

Model.add(layers.Conv2D(filters=32, kernel_size=3, padding='same', input_shape=(128, 128, 1), activation='relu'))
Model.add(layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))
Model.add(layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))
Model.add(layers.MaxPooling2D(pool_size=(2, 2)))


Model.add(layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu'))
Model.add(layers.MaxPooling2D(pool_size=(2, 2)))



Model.add(layers.Flatten())
Model.add(layers.Dense(512, activation='relu'))
Model.add(layers.Dense(128, activation='relu'))
Model.add(layers.BatchNormalization())

Model.add(layers.Dense(64, activation='relu'))
Model.add(layers.Dense(32, activation='relu'))
Model.add(layers.Dropout(0.1))
Model.add(layers.Dense(2, activation='relu'))

Model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy'])
Model.summary()



#splitting training and test
x_train, x_test, y_train, y_test = train_test_split(np.array(X_img), np.array(Y_img), test_size=0.10)

x_train = tf.convert_to_tensor(x_train)
x_train = tf.reshape(x_train, shape=(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1))

x_test = tf.convert_to_tensor(x_test)
x_test = tf.reshape(x_test, shape=(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1))

y_train = tf.convert_to_tensor(y_train)

y_test = tf.convert_to_tensor(y_test)



history = Model.fit(x=x_train, y=y_train, validation_split=0.05,
                    epochs=20, batch_size=16, callbacks=[earlystopping])

Model.evaluate(x_test, y_test)



"""**Methods to improve accuracy**


*  Try out different loss functions, optimizers,  learning rates and activation function

*  Experiment with different types of network architectures, dense layers and number of neurons

* We can use keras tuner to try out different combinations which can increase validation accuracy

* We can also use also data augmentation  to increase the size of dataset by adding some random noise

**Test accuracy** 

1.   Male count accuracy : 0.87
2.   Female count accuracy : 0.85



"""